{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.stats import pearsonr\n",
    "import math,pickle\n",
    "import itertools\n",
    "import glob, os\n",
    "from subprocess import Popen, PIPE\n",
    "import PIL\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "import scipy.signal \n",
    "import scipy.linalg\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('ticks')\n",
    "%matplotlib inline\n",
    "s=\"\"\"0.00025699\n",
    "0.00850739\n",
    "0.06541583\n",
    "0.0784609 \n",
    "0.07641301\n",
    "0.06659586\n",
    "0.05790289\n",
    "0.04679429\n",
    "0.02320798\n",
    "0.01445644\n",
    "0.00695772\n",
    "0.00526551\n",
    "0.002995  \n",
    "0.0019852 \n",
    "0.00128512\n",
    "0.00134175\n",
    "0.00040317\n",
    "0\n",
    "\"\"\"\n",
    "GCaMP6s = np.array(s.replace(\"[\",\"\").replace(\"]\",\"\").split(),dtype=float)\n",
    "\n",
    "def axcov(data, maxlag=5):\n",
    "    \"\"\"\n",
    "    Compute the autocovariance of data at lag = -maxlag:0:maxlag\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array\n",
    "        Array containing fluorescence data\n",
    "    maxlag : int\n",
    "        Number of lags to use in autocovariance calculation\n",
    "    Returns\n",
    "    -------\n",
    "    axcov : array\n",
    "        Autocovariances computed from -maxlag:0:maxlag\n",
    "    \"\"\"\n",
    "    \n",
    "    data = data - np.mean(data)\n",
    "    T = len(data)\n",
    "    bins = np.size(data)\n",
    "    xcov = np.fft.fft(data, np.power(2, nextpow2(2 * bins - 1)))\n",
    "    xcov = np.fft.ifft(np.square(np.abs(xcov)))\n",
    "    xcov = np.concatenate([xcov[np.arange(xcov.size - maxlag, xcov.size)],\n",
    "                           xcov[np.arange(0, maxlag + 1)]])\n",
    "    #xcov = xcov/np.concatenate([np.arange(T-maxlag,T+1),np.arange(T-1,T-maxlag-1,-1)])\n",
    "    return np.real(xcov/T)\n",
    "    \n",
    "def nextpow2(value):\n",
    "    \"\"\"\n",
    "    Find exponent such that 2^exponent is equal to or greater than abs(value).\n",
    "    Parameters\n",
    "    ----------\n",
    "    value : int\n",
    "    Returns\n",
    "    -------\n",
    "    exponent : int\n",
    "    \"\"\"\n",
    "    \n",
    "    exponent = 0\n",
    "    avalue = np.abs(value)\n",
    "    while avalue > np.power(2, exponent):\n",
    "        exponent += 1\n",
    "    return exponent        \n",
    "\n",
    "def estimate_time_constant(fluor, p = 2, sn = None, lags = 5, fudge_factor = 1.):\n",
    "    \"\"\"    \n",
    "    Estimate AR model parameters through the autocovariance function    \n",
    "    Inputs\n",
    "    ----------\n",
    "    fluor        : nparray\n",
    "        One dimensional array containing the fluorescence intensities with\n",
    "        one entry per time-bin.\n",
    "    p            : positive integer\n",
    "        order of AR system  \n",
    "    sn           : float\n",
    "        noise standard deviation, estimated if not provided.\n",
    "    lags         : positive integer\n",
    "        number of additional lags where he autocovariance is computed\n",
    "    fudge_factor : float (0< fudge_factor <= 1)\n",
    "        shrinkage factor to reduce bias\n",
    "        \n",
    "    Return\n",
    "    -----------\n",
    "    g       : estimated coefficients of the AR process\n",
    "    \"\"\"    \n",
    "    \n",
    "\n",
    "    if sn is None:\n",
    "        sn = GetSn(fluor)\n",
    "        \n",
    "    lags += p\n",
    "    xc = axcov(fluor,lags)        \n",
    "    xc = xc[:,np.newaxis]\n",
    "    \n",
    "    A = scipy.linalg.toeplitz(xc[lags+np.arange(lags)],xc[lags+np.arange(p)]) - sn**2*np.eye(lags,p)\n",
    "    g = np.linalg.lstsq(A,xc[lags+1:])[0]\n",
    "    gr = np.roots(np.concatenate([np.array([1]),-g.flatten()]))\n",
    "    gr = (gr+gr.conjugate())/2.\n",
    "    gr[gr>1] = 0.95 + np.random.normal(0,0.01,np.sum(gr>1))\n",
    "    gr[gr<0] = 0.15 + np.random.normal(0,0.01,np.sum(gr<0))\n",
    "    g = np.poly(fudge_factor*gr)\n",
    "    g = -g[1:]    \n",
    "        \n",
    "    return g.flatten()\n",
    "    \n",
    "\n",
    "def estimate_parameters(fluor, p = 2, sn = None, g = None, range_ff = [0.25,0.5], method = 'logmexp', lags = 5, fudge_factor = 1):\n",
    "    \"\"\"\n",
    "    Estimate noise standard deviation and AR coefficients if they are not present\n",
    "    p: positive integer\n",
    "        order of AR system  \n",
    "    sn: float\n",
    "        noise standard deviation, estimated if not provided.\n",
    "    lags: positive integer\n",
    "        number of additional lags where he autocovariance is computed\n",
    "    range_ff : (1,2) array, nonnegative, max value <= 0.5\n",
    "        range of frequency (x Nyquist rate) over which the spectrum is averaged  \n",
    "    method: string\n",
    "        method of averaging: Mean, median, exponentiated mean of logvalues (default)\n",
    "    fudge_factor: float (0< fudge_factor <= 1)\n",
    "        shrinkage factor to reduce bias\n",
    "    \"\"\"\n",
    "    \n",
    "    if sn is None:\n",
    "        sn = GetSn(fluor,range_ff,method)\n",
    "        \n",
    "    if g is None:\n",
    "        if p == 0:\n",
    "            g = np.array(0)\n",
    "        else:\n",
    "            g = estimate_time_constant(fluor,p,sn,lags,fudge_factor)\n",
    "\n",
    "    return g,sn\n",
    "\n",
    "def GetSn(fluor, range_ff = [0.25,0.5], method = 'logmexp'):\n",
    "    \"\"\"    \n",
    "    Estimate noise power through the power spectral density over the range of large frequencies    \n",
    "    Inputs\n",
    "    ----------\n",
    "    fluor    : nparray\n",
    "        One dimensional array containing the fluorescence intensities with\n",
    "        one entry per time-bin.\n",
    "    range_ff : (1,2) array, nonnegative, max value <= 0.5\n",
    "        range of frequency (x Nyquist rate) over which the spectrum is averaged  \n",
    "    method   : string\n",
    "        method of averaging: Mean, median, exponentiated mean of logvalues (default)\n",
    "        \n",
    "    Return\n",
    "    -----------\n",
    "    sn       : noise standard deviation\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    ff, Pxx = scipy.signal.welch(fluor)\n",
    "    ind1 = ff > range_ff[0]\n",
    "    ind2 = ff < range_ff[1]\n",
    "    ind = np.logical_and(ind1,ind2)\n",
    "    Pxx_ind = Pxx[ind]\n",
    "    sn = {\n",
    "        'mean': lambda Pxx_ind: np.sqrt(np.mean(Pxx_ind/2)),\n",
    "        'median': lambda Pxx_ind: np.sqrt(np.median(Pxx_ind/2)),\n",
    "        'logmexp': lambda Pxx_ind: np.sqrt(np.exp(np.mean(np.log(Pxx_ind/2))))\n",
    "    }[method](Pxx_ind)\n",
    "\n",
    "    return sn\n",
    "\n",
    "\n",
    "def cvxpy_foopsi(fluor,  g=None, sn=None, b=None, c1=None, bas_nonneg=True,solvers=None):\n",
    "    '''Solves the deconvolution problem using the cvxpy package and the ECOS/SCS library. \n",
    "    Parameters:\n",
    "    -----------\n",
    "    fluor: ndarray\n",
    "        fluorescence trace \n",
    "    g: list of doubles\n",
    "        parameters of the autoregressive model, cardinality equivalent to p        \n",
    "    sn: double\n",
    "        estimated noise level\n",
    "    b: double\n",
    "        baseline level. If None it is estimated. \n",
    "    c1: double\n",
    "        initial value of calcium. If None it is estimated.  \n",
    "    bas_nonneg: boolean\n",
    "        should the baseline be estimated        \n",
    "    solvers: tuple of two strings\n",
    "        primary and secondary solvers to be used. Can be choosen between ECOS, SCS, CVXOPT    \n",
    "    Returns:\n",
    "    --------\n",
    "    c: estimated calcium trace\n",
    "    b: estimated baseline\n",
    "    c1: esimtated initial calcium value\n",
    "    g: esitmated parameters of the autoregressive model\n",
    "    sn: estimated noise level\n",
    "    sp: estimated spikes \n",
    "        \n",
    "    '''\n",
    "    if g is None or sn is None:        \n",
    "        g,sn = estimate_parameters(fluor, p=2, sn=sn, g = g, range_ff=noise_range, method=noise_method, lags=lags, fudge_factor=1)\n",
    "    try:\n",
    "        import cvxpy as cvx\n",
    "    except ImportError:\n",
    "        raise ImportError('cvxpy solver requires installation of cvxpy.')\n",
    "    if solvers is None:\n",
    "        solvers=['ECOS','SCS']\n",
    "        \n",
    "    T = fluor.size\n",
    "    \n",
    "    # construct deconvolution matrix  (sp = G*c)     \n",
    "    G=scipy.sparse.dia_matrix((np.ones((1,T)),[0]),(T,T))\n",
    "    \n",
    "    for i,gi in enumerate(g):\n",
    "        G = G + scipy.sparse.dia_matrix((-gi*np.ones((1,T)),[-1-i]),(T,T))\n",
    "        \n",
    "    gr = np.roots(np.concatenate([np.array([1]),-g.flatten()])) \n",
    "    gd_vec = np.max(gr)**np.arange(T)  # decay vector for initial fluorescence\n",
    "    gen_vec = G.dot(scipy.sparse.coo_matrix(np.ones((T,1))))                          \n",
    " \n",
    "    c = cvx.Variable(T) # calcium at each time step\n",
    "    constraints=[]\n",
    "    cnt = 0\n",
    "    if b is None:\n",
    "        flag_b = True\n",
    "        cnt += 1\n",
    "        b =  cvx.Variable(1) # baseline value\n",
    "        if bas_nonneg:\n",
    "            b_lb = 0\n",
    "        else:\n",
    "            b_lb = np.min(fluor)            \n",
    "        constraints.append(b >= b_lb)\n",
    "    else:\n",
    "        flag_b = False\n",
    "\n",
    "    if c1 is None:\n",
    "        flag_c1 = True\n",
    "        cnt += 1\n",
    "        c1 =  cvx.Variable(1) # baseline value\n",
    "        constraints.append(c1 >= 0)\n",
    "    else:\n",
    "        flag_c1 = False    \n",
    "    \n",
    "    thrNoise=sn * np.sqrt(fluor.size)\n",
    "    \n",
    "    try:\n",
    "        objective=cvx.Minimize(cvx.norm(G*c,1)) # minimize number of spikes\n",
    "        constraints.append(G*c >= 0)\n",
    "        constraints.append(cvx.norm(-c + fluor - b - gd_vec*c1, 2) <= thrNoise) # constraints\n",
    "        prob = cvx.Problem(objective, constraints) \n",
    "        result = prob.solve(solver=solvers[0])    \n",
    "        \n",
    "        if  not (prob.status ==  'optimal' or prob.status == 'optimal_inaccurate'):\n",
    "            raise ValueError('Problem solved suboptimally or unfeasible')            \n",
    "        \n",
    "#         print 'PROBLEM STATUS:' + prob.status \n",
    "#         sys.stdout.flush()\n",
    "    except (ValueError,cvx.SolverError) as err:     # if solvers fail to solve the problem           \n",
    "#          print(err) \n",
    "#          sys.stdout.flush()\n",
    "         lam=sn/500;\n",
    "         constraints=constraints[:-1]\n",
    "         objective = cvx.Minimize(cvx.norm(-c + fluor - b - gd_vec*c1, 2)+lam*cvx.norm(G*c,1))\n",
    "         prob = cvx.Problem(objective, constraints)\n",
    "         try: #in case scs was not installed properly\n",
    "             try:\n",
    "#                  print('TRYING AGAIN ECOS') \n",
    "#                  sys.stdout.flush()\n",
    "                 result = prob.solve(solver=solvers[0]) \n",
    "             except:\n",
    "                 result = prob.solve(solver=solvers[1]) \n",
    "         except:             \n",
    "#              sys.stderr.write('***** SCS solver failed, try installing and compiling SCS for much faster performance. Otherwise set the solvers in tempora_params to [\"ECOS\",\"CVXOPT\"]')\n",
    "#              sys.stderr.flush()\n",
    "             result = prob.solve(solver='CVXOPT')\n",
    "             raise\n",
    "             \n",
    "         if not (prob.status ==  'optimal' or prob.status == 'optimal_inaccurate'):\n",
    "#             print 'PROBLEM STATUS:' + prob.status \n",
    "            raise Exception('Problem could not be solved')\n",
    "            \n",
    "    \n",
    "    \n",
    "    sp = np.squeeze(np.asarray(G*c.value))    \n",
    "    c = np.squeeze(np.asarray(c.value))                \n",
    "    if flag_b:    \n",
    "        b = np.squeeze(b.value)        \n",
    "    if flag_c1:    \n",
    "        c1 = np.squeeze(c1.value)\n",
    "        \n",
    "    return c,b,c1,g,sn,sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def constrained_foopsi_parallel(fluor):\n",
    "    cc_,cb_,c1_,gn_,sn_,sp_ = cvxpy_foopsi(fluor)\n",
    "    return cc_,cb_,c1_,gn_,sn_,sp_\n",
    "from joblib import Parallel,delayed\n",
    "noise_method = 'logmexp'\n",
    "noise_range = [.25,.5]\n",
    "pool = Parallel(n_jobs=11, verbose=2)\n",
    "lags = 5 \n",
    "fudge_factor = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory='Emmanuel'\n",
    "savedirectory='Emmanuel/Results'\n",
    "os.chdir('/mnt/downloads/'+directory+'/')\n",
    "p=Popen(['ls'], shell=False, stdout=PIPE, close_fds=True).stdout.readlines()\n",
    "filelist=[]\n",
    "for filename in p:\n",
    "    if filename.startswith('fish'):\n",
    "        if filename.endswith('tif\\n'):\n",
    "            filelist.append(filename.rstrip('\\n'))\n",
    "            \n",
    "filelist.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fish100_depth4_age12.tif',\n",
       " 'fish101_depth4_age12.tif',\n",
       " 'fish102_depth4_age12.tif',\n",
       " 'fish103_depth4_age6.tif',\n",
       " 'fish104_depth4_age6.tif',\n",
       " 'fish105_depth4_age6.tif',\n",
       " 'fish106_depth4_age6.tif',\n",
       " 'fish107_depth4_age9.tif',\n",
       " 'fish108_depth4_age9 second.tif',\n",
       " 'fish108_depth4_age9.tif',\n",
       " 'fish110_depth4_age6 second.tif',\n",
       " 'fish110_depth4_age6.tif',\n",
       " 'fish111_depth4_age6.tif',\n",
       " 'fish66_depth4_age6.tif',\n",
       " 'fish67_depth4_age6.tif',\n",
       " 'fish68_depth4_age6.tif',\n",
       " 'fish71_depth4_age12.tif',\n",
       " 'fish72_depth4_age6.tif',\n",
       " 'fish73_depth4_age6.tif',\n",
       " 'fish80_depth4_age12.tif',\n",
       " 'fish81_depth4_age12.tif',\n",
       " 'fish82_depth4_age12.tif',\n",
       " 'fish84_depth4_age6.tif',\n",
       " 'fish84_depth4_age6_second.tif',\n",
       " 'fish85_depth4_age6.tif',\n",
       " 'fish86_depth4_age6.tif',\n",
       " 'fish89_depth4_age9.tif',\n",
       " 'fish93_depth4_age12.tif',\n",
       " 'fish94_depth4_age12.tif',\n",
       " 'fish95_depth4_age9.tif',\n",
       " 'fish96_depth4_age12.tif',\n",
       " 'fish98_depth4_age12.tif',\n",
       " 'fish99_depth4_age12.tif']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "counter=-1\n",
    "dict_allindex=dict.fromkeys(filelist, 0)\n",
    "#import skimage.external.tifffile as tiff\n",
    "\n",
    "\n",
    "for i,filename in enumerate(filelist):  \n",
    "    Mask = PIL.Image.open('/mnt/downloads/'+directory+'/Mask_AVG_'+filename)\n",
    "    Mask=np.asarray(Mask,dtype=np.int64)    \n",
    "    for value in xrange(0,Mask.max()+1):\n",
    "        size=(Mask==value).sum()\n",
    "        if 8 < size < 200:\n",
    "            Mask[Mask==value]=counter\n",
    "            counter=counter-1\n",
    "        else:\n",
    "            Mask[Mask==value]=0\n",
    "    Mask=np.absolute(Mask)\n",
    "    Mask.astype(np.uint32)\n",
    "    tiff.imsave('/mnt/downloads/'+savedirectory+'/Maskb_'+filename, Mask)\n",
    "    MeanFluo_ROI=tiff.imread('/mnt/downloads/'+directory+'/'+filename)\n",
    "    MeanFluo_ROI=MeanFluo_ROI.swapaxes(0,2).swapaxes(0,1)\n",
    "    uniq=np.unique(Mask)\n",
    "    meanVals=[np.mean(MeanFluo_ROI[Mask == grp],axis=0) for grp in uniq if grp != 0]\n",
    "    meanVals=np.asarray(meanVals,dtype=np.uint16)\n",
    "    Centroids=[Parallel(n_jobs=-1)(delayed(scipy.ndimage.measurements.center_of_mass)(Mask,Mask,index=grp) for grp in uniq if grp !=0)]\n",
    "    Centroids=np.asarray(Centroids).squeeze()    \n",
    "    scipy.io.savemat('/mnt/downloads/'+savedirectory+'/'+filename+'-FluoTraces-Maskb.mat', mdict={'Centroids':Centroids,'FluoTraces':meanVals}, oned_as='column')\n",
    "    if i==0 :\n",
    "        Centroids_ROI=Centroids\n",
    "        SegmentData=meanVals\n",
    "        dict_allindex[filename]=SegmentData.shape[0]\n",
    "    else:\n",
    "        Centroids_ROI=np.concatenate((Centroids_ROI,Centroids),axis=0)\n",
    "        SegmentData=np.concatenate((SegmentData,meanVals),axis=0)\n",
    "        dict_allindex[filename]=SegmentData.shape[0]\n",
    "    if meanVals.shape[0]+1 != np.unique(Mask).shape[0]:\n",
    "        print \"there was an error with the ROIs\"\n",
    "        break\n",
    "    if np.absolute(counter+1) != SegmentData.shape[0]:\n",
    "        print \"there was an error with the counter\"\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4679, 700)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SegmentData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.io.savemat('/mnt/downloads/'+savedirectory+'/'+directory+'-FluoTraces-Maskb.mat', mdict={'Centroids':Centroids_ROI,'FluoTraces':SegmentData}, oned_as='column')\n",
    "with open('/mnt/downloads/'+savedirectory+'/'+directory+'-dict_allindex.pickle', 'w') as handle:\n",
    "      pickle.dump(dict_allindex, handle)    \n",
    "filelist=np.zeros(len(dict_allindex.keys()),dtype=np.object)\n",
    "values=np.zeros(len(dict_allindex.keys()),dtype=int)\n",
    "i=0\n",
    "for key in sorted(dict_allindex):\n",
    "    filelist[i]=key\n",
    "    values[i]=dict_allindex[key]\n",
    "    i=i+1\n",
    "scipy.io.savemat('/mnt/downloads/'+savedirectory+'/'+directory+'-dict_allindex_Maskb.mat',mdict={'files':filelist,'index':values}, oned_as='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud8freq=np.zeros((8,700),dtype=np.float16);\n",
    "aud8freq[0,21:21+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud8freq[0,407:407+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud8freq[0,612:612+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "\n",
    "aud8freq[1,47:47+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud8freq[1,382:382+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud8freq[1,532:532+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "\n",
    "aud8freq[2,71:71+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud8freq[2,356:356+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud8freq[2,507:507+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "\n",
    "aud8freq[3,96:96+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud8freq[3,331:331+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud8freq[3,557:557+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "\n",
    "aud8freq[4,121:121+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud8freq[4,308:308+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud8freq[4,487:487+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "\n",
    "aud8freq[5,146:146+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud8freq[5,281:281+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud8freq[5,586:586+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "\n",
    "aud8freq[6,175:175+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud8freq[6,256:256+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud8freq[6,461:461+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "\n",
    "aud8freq[7,200:200+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud8freq[7,231:231+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud8freq[7,636:636+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "\n",
    "row,col=np.where(aud8freq==0.00850739000000000)\n",
    "corr=np.zeros(SegmentData.shape[0])\n",
    "for x in enumerate(SegmentData):\n",
    "    for y in col:\n",
    "        for z in xrange(-3,6):\n",
    "            w=y+z;\n",
    "            temp=x[1][w:w+GCaMP6s.shape[0]]\n",
    "            temp=np.corrcoef(temp,GCaMP6s)\n",
    "            if temp[0,1]>0.75:\n",
    "                corr[x[0]]=corr[x[0]]+1\n",
    "                break\n",
    "idx_corr=corr>3\n",
    "SelectCorr=SegmentData[idx_corr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.io.savemat('/mnt/downloads/'+savedirectory+'/GCaMP6f-ganglia-FluoTraces-Maskb-SelectStrict.mat', mdict={'SelectCorr':SelectCorr,'Nb_corr':corr}, oned_as='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.io.savemat('/mnt/downloads/'+savedirectory+'GCaMP6f-'+directory+'_oopsi.mat', mdict={'Sp_infer':sp_,'Ca_infer':cc_,'baseline_infer':cb_,'Ca_init':c1_,'Params_est':gn_,'Noise':sn_}, oned_as='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/numpy/matrixlib/defmatrix.py:262: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return new.astype(intype)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/numpy/matrixlib/defmatrix.py:262: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return new.astype(intype)\n",
      "[Parallel(n_jobs=11)]: Done  19 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=11)]: Done 140 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=11)]: Done 343 tasks      | elapsed:    4.6s\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/numpy/matrixlib/defmatrix.py:262: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return new.astype(intype)\n",
      "[Parallel(n_jobs=11)]: Done 626 tasks      | elapsed:    7.7s\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/numpy/matrixlib/defmatrix.py:262: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return new.astype(intype)\n",
      "[Parallel(n_jobs=11)]: Done 991 tasks      | elapsed:   12.1s\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/numpy/matrixlib/defmatrix.py:262: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return new.astype(intype)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/numpy/matrixlib/defmatrix.py:262: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return new.astype(intype)\n",
      "[Parallel(n_jobs=11)]: Done 1436 tasks      | elapsed:   17.1s\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/numpy/matrixlib/defmatrix.py:262: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return new.astype(intype)\n",
      "[Parallel(n_jobs=11)]: Done 1963 tasks      | elapsed:   23.1s\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/numpy/matrixlib/defmatrix.py:262: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return new.astype(intype)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/numpy/matrixlib/defmatrix.py:262: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return new.astype(intype)\n",
      "[Parallel(n_jobs=11)]: Done 2570 tasks      | elapsed:   32.3s\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/numpy/matrixlib/defmatrix.py:262: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return new.astype(intype)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/numpy/matrixlib/defmatrix.py:262: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return new.astype(intype)\n",
      "[Parallel(n_jobs=11)]: Done 3259 tasks      | elapsed:   42.3s\n",
      "[Parallel(n_jobs=11)]: Done 4028 tasks      | elapsed:   55.2s\n",
      "[Parallel(n_jobs=11)]: Done 4679 out of 4679 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/downloads/ptf1a/resultsGCaMP6f-ptf1a/results_oopsi.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e568dafa14fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstrained_foopsi_parallel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSegmentData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcc_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcb_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc1_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgn_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msn_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msp_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavemat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/mnt/downloads/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msavedirectory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'GCaMP6f-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msavedirectory\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_oopsi.mat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Sp_infer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msp_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Ca_infer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcc_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'baseline_infer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcb_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Ca_init'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mc1_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Params_est'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgn_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Noise'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msn_\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moned_as\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'column'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/scipy/io/matlab/mio.pyc\u001b[0m in \u001b[0;36msavemat\u001b[0;34m(file_name, mdict, appendmat, format, long_field_names, do_compression, oned_as)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mfile_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'write'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/mnt/downloads/ptf1a/resultsGCaMP6f-ptf1a/results_oopsi.mat'"
     ]
    }
   ],
   "source": [
    "#SegmentData=np.transpose(SegmentData)\n",
    "import scipy.io\n",
    "results = pool(delayed(constrained_foopsi_parallel)(rr) for rr in SegmentData)\n",
    "cc_,cb_,c1_,gn_,sn_,sp_ = zip(*results)\n",
    "scipy.io.savemat('/mnt/downloads/'+savedirectory+'GCaMP6f-'+savedirectory+'_oopsi.mat', mdict={'Sp_infer':sp_,'Ca_infer':cc_,'baseline_infer':cb_,'Ca_init':c1_,'Params_est':gn_,'Noise':sn_}, oned_as='column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud5vol=np.zeros((5,550),dtype=np.float16);\n",
    "aud5vol[0,51:51+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud5vol[0,301:301+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud5vol[0,376:376+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud5vol[1,76:76+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud5vol[1,276:276+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud5vol[1,451:451+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud5vol[2,101:101+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud5vol[2,251:251+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud5vol[2,426:426+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud5vol[3,126:126+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud5vol[3,226:226+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud5vol[3,401:401+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud5vol[4,151:151+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud5vol[4,201:201+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "aud5vol[4,351:351+GCaMP6s.shape[0]]=GCaMP6s;\n",
    "\n",
    "row,col=np.where(aud5vol==0.00850739000000000)\n",
    "corr=np.zeros(SegmentData.shape[0])\n",
    "for x in enumerate(SegmentData):\n",
    "    for y in col:\n",
    "        for z in xrange(-3,6):\n",
    "            w=y+z;\n",
    "            temp=x[1][w:w+GCaMP6s.shape[0]]\n",
    "            temp=np.corrcoef(temp,GCaMP6s)\n",
    "            if temp[0,1]>0.75:\n",
    "                corr[x[0]]=corr[x[0]]+1\n",
    "                break\n",
    "idx_corr=corr>3\n",
    "SelectCorr=SegmentData[idx_corr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    " \n",
    "import sys\n",
    "import numpy as np\n",
    "import ca_source_extraction as cse\n",
    "import gc, pickle\n",
    "\n",
    "from time import time\n",
    "from scipy.sparse import coo_matrix\n",
    "import tifffile\n",
    "import subprocess\n",
    "import time as tm\n",
    "from time import time\n",
    "import pylab as pl\n",
    "import psutil\n",
    "import glob\n",
    "import os\n",
    "import scipy\n",
    "#%%\n",
    "\n",
    "#%% FOR LOADING ALL TIFF FILES IN A FILE AND SAVING THEM ON A SINGLE MEMORY MAPPABLE FILE\n",
    "fnames_all=[]\n",
    "base_folder='/mnt/downloads/Lucy/' # folder containing the demo files\n",
    "for file in glob.glob(os.path.join(base_folder,'f*.tif')):\n",
    "    if file.endswith(\".tif\"):\n",
    "        fnames_all.append(file)\n",
    "fnames_all.sort()\n",
    "#%% stop server and remove log files\n",
    "cse.utilities.stop_server() \n",
    "log_files=glob.glob('Yr*_LOG_*')\n",
    "for log_file in log_files:\n",
    "    os.remove(log_file)\n",
    "from ipyparallel import Client\n",
    "n_processes = 11\n",
    "sys.stdout.flush()  \n",
    "cse.utilities.stop_server()\n",
    "cse.utilities.start_server()\n",
    "c=Client()\n",
    "dview=c[:n_processes]\n",
    "import gc\n",
    "import scipy.io\n",
    "# n_processes = 12\n",
    "p=2\n",
    "gSig=[3,3]\n",
    "K=500\n",
    "# K=250\n",
    "# cse.utilities.stop_server() # trying to stop in case it was already runnning\n",
    "# cse.utilities.start_server(n_processes)\n",
    "# with open('/mnt/downloads/Hindbrain-dict_allindex.pickle', 'r') as handle:\n",
    "#     dict_allindex=pickle.load(handle)\n",
    "a=-1\n",
    "for i,name in enumerate(fnames_all):\n",
    "    if i>a:\n",
    "#         cse.utilities.start_server(n_processes)\n",
    "        fnames=[name]\n",
    "        fname_new=cse.utilities.save_memmap(fnames,base_name='Yr',resize_fact=(1,1,1))\n",
    "        Yr,dims,T=cse.utilities.load_memmap(fname_new)\n",
    "#         d,T=np.shape(Yr)\n",
    "        Y=np.reshape(Yr,dims+(T,),order='F')\n",
    "        options = cse.utilities.CNMFSetParms(Y,n_processes,p=p,gSig=gSig,K=K,tsub=3)\n",
    "        Cn = cse.utilities.local_correlations(Y[:1000])    \n",
    "        Yr,sn,g,psx = cse.pre_processing.preprocess_data(Yr, dview=dview,**options['preprocess_params'])\n",
    "        Ain,Cin, b_in, f_in, center=cse.initialization.initialize_components(Y,sn=sn, **options['init_params'])\n",
    "        if np.isfinite(Cin).max(): \n",
    "            options['temporal_params']['p'] = 0 # set this to zero for fast updating without deconvolution\n",
    "            Cin,f,S,bl,c1,neurons_sn,g,YrA = cse.temporal.update_temporal_components(Yr,Ain,b_in,Cin,f_in,dview=dview,bl=None,c1=None,sn=None,g=None,**options['temporal_params'])\n",
    "            #%% UPDATE SPATIAL COMPONENTS\n",
    "            Ain,b_in,Cin = cse.spatial.update_spatial_components(Yr, Cin, f_in, Ain, sn=sn, dview=dview, **options['spatial_params'])\n",
    "            options['temporal_params']['p'] = 0 # set this to zero for fast updating without deconvolution\n",
    "            Cin,f,S,bl,c1,neurons_sn,g,YrA = cse.temporal.update_temporal_components(Yr,Ain,b_in,Cin,f_in,bl=None,c1=None,sn=None,g=None,**options['temporal_params'])\n",
    "            Ain,Cin,nr_m,merged_ROIs,S_m,bl_m,c1_m,sn_m,g_m=cse.merging.merge_components(Yr,Ain,b_in,Cin,f,S,sn,options['temporal_params'], options['spatial_params'], bl=bl, c1=c1, sn=neurons_sn, g=g, thr=0.8,mx=1000, fast_merge = True)\n",
    "            Ain,b_in,Cin = cse.spatial.update_spatial_components(Yr, Cin, f, Ain, dview=dview, sn=sn, **options['spatial_params'])\n",
    "            options['temporal_params']['p'] = p # set it back to original value to perform full deconvolution\n",
    "            Cin,f2,S,bl2,c12,neurons_sn2,g21,YrA = cse.temporal.update_temporal_components(Yr,Ain,b_in,Cin,f, dview=dview,bl=None,c1=None,sn=None,g=None,**options['temporal_params'])    \n",
    "            traces=Cin+YrA\n",
    "            idx_components, fitness, erfc = cse.utilities.evaluate_components(traces,N=5,robust_std=True)\n",
    "            scipy.io.savemat(fnames[0][:-4]+'_output_analysis_matlab.mat',mdict={'ROIs':Ain,'DenoisedTraces':Cin, 'Noise':YrA, 'Spikes': S , 'idx_components':idx_components, 'fitness':fitness})\n",
    "        #             dict_allindex[name]=Cin.shape[0]\n",
    "        #         if name == fnames_all[0]:\n",
    "        #             Spikes=S\n",
    "        #             Calcium=Cin\n",
    "        #         else:\n",
    "        #             Spikes=np.vstack((Spikes,S))\n",
    "        #             Calcium=np.vstack((Calcium,Cin))\n",
    "#             cse.utilities.stop_server()\n",
    "            log_files=glob.glob('Yr*_LOG_*')\n",
    "            for log_file in log_files:\n",
    "                os.remove(log_file)\n",
    "            del(Y,YrA)\n",
    "            del(Yr,Ain,S,Cin)\n",
    "        #         np.savez('/mnt/downloads/Temp.npz',Spikes=Spikes,Calcium=Calcium,i=i,name=name)\n",
    "        #             with open('/mnt/downloads/Hindbrain-dict_allindex.pickle', 'w') as handle:\n",
    "        #                 pickle.dump(dict_allindex, handle)\n",
    "        np.save('/mnt/downloads/Temp.npy',i)\n",
    "        gc.collect()\n",
    "cse.utilities.stop_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "def compute_bic(kmeans,X):\n",
    "    \"\"\"\n",
    "    Computes the BIC metric for a given clusters\n",
    "\n",
    "    Parameters:\n",
    "    -----------------------------------------\n",
    "    kmeans:  List of clustering object from scikit learn\n",
    "\n",
    "    X     :  multidimension np array of data points\n",
    "\n",
    "    Returns:\n",
    "    -----------------------------------------\n",
    "    BIC value\n",
    "    \"\"\"\n",
    "    # assign centers and labels\n",
    "    centers = kmeans.cluster_centers_\n",
    "    labels  = kmeans.labels_\n",
    "    #number of clusters\n",
    "    m = kmeans.n_clusters\n",
    "    # size of the clusters\n",
    "    n = np.bincount(labels)\n",
    "    #size of data set\n",
    "    N, d = X.shape\n",
    "\n",
    "    #compute variance for all clusters beforehand\n",
    "    cl_var = (1.0 / (N - m) / d) * sum([sum(distance.cdist(X[np.where(labels == i)], [centers[i]], 'euclidean')**2) for i in range(m)])\n",
    "\n",
    "    const_term = 0.5 * m * np.log(N) * (d+1)\n",
    "\n",
    "    BIC = np.sum([n[i] * np.log(n[i]) -\n",
    "               n[i] * np.log(N) -\n",
    "             ((n[i] * d) / 2) * np.log(2*np.pi*cl_var) -\n",
    "             ((n[i] - 1) * d/ 2) for i in range(m)]) - const_term\n",
    "    \n",
    "\n",
    "    return(BIC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MiniBatchKMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b8ba285f16d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmax\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mkmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmbkmv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMiniBatchKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSelectCorr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mbic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_bic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmbkmv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSelectCorr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MiniBatchKMeans' is not defined"
     ]
    }
   ],
   "source": [
    "kmin=1\n",
    "kmax=200\n",
    "bic=np.zeros(kmax-kmin)\n",
    "for i in xrange(kmin,kmax):\n",
    "    mbkmv = MiniBatchKMeans(i, max_iter=200, batch_size=10000).fit(SelectCorr)\n",
    "    bic[i]=compute_bic(mbkmv,SelectCorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bic[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
